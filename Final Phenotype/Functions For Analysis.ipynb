{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#graph network packages\n",
    "import obonet\n",
    "import networkx as nx\n",
    "\n",
    "#API requests packages\n",
    "import requests\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# For Analysis\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO Annotations File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpo_annotations_and_clean():\n",
    "    \"\"\" This function loads data from the HPO annotations file:  \n",
    "    http://compbio.charite.de/jenkins/job/hpo.annotations/lastStableBuild/artifact/misc/phenotype_annotation.tab\n",
    "    \n",
    "    It then uses the inner function **get_references_one_per_row function** to ensure that references,\n",
    "    which are the sources indicated for the HPO code, are each represented in their own row\n",
    "\n",
    "    Input: None\n",
    "    Output: Dataset from HPO with thier annotations for each disease\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_references_one_per_row (dataset):\n",
    "\n",
    "        semicolon = dataset['reference'].str.split(\";\", n = 0, expand = True)\n",
    "        semicolon_data = pd.concat([dataset['reference'], semicolon], axis=1)\n",
    "\n",
    "        semi_done = pd.DataFrame()\n",
    "        for i in range(1,len(semicolon_data.columns)):\n",
    "            one = semicolon_data.iloc[:,[0,i]]\n",
    "            one.columns = ['reference', 'one_reference'] \n",
    "\n",
    "            semi_done = semi_done.append(one)\n",
    "\n",
    "        semi_done = semi_done[(semi_done['one_reference'] != '')]\n",
    "        semi_done = semi_done[~semi_done['one_reference'].isnull()]\n",
    "\n",
    "    #Repeating first step with comma.  Should be one function\n",
    "        semicolon_comma = semi_done['one_reference'].str.split(\",\", n = 0, expand = True)\n",
    "\n",
    "        semicolon_comma_data = pd.concat([semi_done['reference'], semicolon_comma], axis=1)\n",
    "\n",
    "        semi_comma_done = pd.DataFrame()\n",
    "        for i in range(1,len(semicolon_comma_data.columns)):\n",
    "            one = semicolon_comma_data.iloc[:,[0,i]]\n",
    "            one.columns = ['reference', 'one_reference'] \n",
    "\n",
    "            semi_comma_done = semi_comma_done.append(one)\n",
    "\n",
    "        semi_comma_done = semi_comma_done[(semi_comma_done['one_reference'] != '')]\n",
    "        semi_comma_done = semi_comma_done[~semi_comma_done['one_reference'].isnull()]\n",
    "\n",
    "        final_data = semi_comma_done.drop_duplicates()\n",
    "        return final_data\n",
    "    \n",
    "    hpo_file = pd.read_table('http://compbio.charite.de/jenkins/job/hpo.annotations/lastStableBuild/artifact/misc/phenotype_annotation.tab')\n",
    "\n",
    "    corrected_references = get_references_one_per_row(hpo_file)\n",
    "\n",
    "    final_data = hpo_file.merge(corrected_references, how = 'left', on = 'reference')\n",
    "    final_data['reference'] = final_data['one_reference']\n",
    "\n",
    "    final_data = final_data.drop(['one_reference'], axis=1)\n",
    "\n",
    "#ADDING THESE COLUMNS into the cleaning process\n",
    "    final_data = final_data.rename(columns = {'HPO-ID': 'hpo'})\n",
    "    final_data['uniqueid'] = final_data['hpo'] + final_data['reference']\n",
    "\n",
    "\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpo_annotations_pmid_and_disease(hpo_annotations_dataset):\n",
    "    \"\"\"  This function selects all of the pubmed ids from the hpo annotations dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    hpo_annotations_dataset['text_id'] = hpo_annotations_dataset['reference']\n",
    "    \n",
    "    #We only want the pubmed articles and thier related diseases\n",
    "    direct_annotations = hpo_annotations_dataset[hpo_annotations_dataset['text_id'].str.contains('PMID:', na = False)]\n",
    "    direct_annotations['text_id'] = direct_annotations['text_id'].str.replace('PMID:', '')\n",
    "\n",
    "\n",
    "    each_unique_pair = direct_annotations[['disease-name', 'text_id']].drop_duplicates()\n",
    "\n",
    "    #Some pubmeds reference more than one disease.  We want to make sure we remove\n",
    "    #those pubmeds\n",
    "    checking_singilarity = each_unique_pair.groupby(['text_id']).count().reset_index()\n",
    "    two_count = checking_singilarity[checking_singilarity['disease-name'] > 1]\n",
    "    \n",
    "    final_list_for_diseases = each_unique_pair[~each_unique_pair['text_id'].isin(two_count['text_id'])]\n",
    "\n",
    "    \n",
    "    return final_list_for_diseases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for HPO Graph (OBO) File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpo_hierarchy_graph_load():\n",
    "    \"\"\"Loads all data from the graph/obo file located here:\n",
    "\n",
    "    https://raw.githubusercontent.com/obophenotype/human-phenotype-ontology/master/hp.obo\n",
    "    \n",
    "    Input: None\n",
    "\n",
    "    Output: HPO IDs and their hierarchy\n",
    "    \"\"\"\n",
    "    url = 'https://raw.githubusercontent.com/obophenotype/human-phenotype-ontology/master/hp.obo'\n",
    "    graph = obonet.read_obo(url)\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_phenotypic_abnormality(graph_network):\n",
    "    \"\"\"This creates a list of HP codes that are children of the phenotypic abnormality.\n",
    "        It checks if each HPO code in the graph is a child of HP:0000118, which is phenotypic abnormality.\n",
    "\n",
    "        Input: HPO graph_network\n",
    "        Output: List of hpo codes that are under phenotypic abnormality\n",
    "        \n",
    "    \"\"\"\n",
    "    phenotypic_list = []\n",
    "\n",
    "    for i in range(len(list(graph_network.nodes))):\n",
    "    \n",
    "        if  nx.has_path(graph_network, list(graph_network.nodes)[i],'HP:0000118') == True:\n",
    "        \n",
    "            phenotypic_list.append(list(graph_network.nodes)[i])\n",
    "    \n",
    "    return phenotypic_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_external_references_for_hpo_codes(graph_network):\n",
    "    \"\"\" This function pulls all of the other codes associated with each hpo code\n",
    "\n",
    "    Input: HPO graph network\n",
    "    Output: DataFrame with hpo codes and associated external references\n",
    "    \"\"\"\n",
    "    \n",
    "    a = nx.get_node_attributes(graph_network, 'xref')\n",
    "    a_df = pd.DataFrame.from_dict(a,orient='index')\n",
    "    a_df['HPO'] = a_df.index\n",
    "\n",
    "    all_references = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0,(len(a_df.columns)-1)):\n",
    "        one = a_df.iloc[:,[12,i]]\n",
    "        one.columns = ['hpo', 'x_ref'] \n",
    "        \n",
    "        all_references = all_references.append(one)\n",
    "\n",
    "    all_references = all_references[(all_references['x_ref'] != '')]\n",
    "    all_references = all_references[~all_references['x_ref'].isnull()]\n",
    "\n",
    "    all_references = all_references.rename_axis(None, axis = 1)\n",
    "\n",
    "    return all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpos_for_umls_code(graph_network):\n",
    "    \"\"\"Metamap is coded under UMLS concept codes.  We need to convert these into HPO code.\n",
    "    There are external references for HPOs codes in the graph network of HPO phenotypes.\n",
    "    \n",
    "    This function gets all of the umls codes for hpos that are phenotypic abnormalities.\n",
    "    \n",
    "    Child Function: get_all_external_references_for_hpo_codes\n",
    "    \"\"\"\n",
    "\n",
    "    all_references = get_all_external_references_for_hpo_codes(graph_network)\n",
    "    umls_references = all_references[all_references['x_ref'].str.contains('UMLS')]\n",
    "    umls_references['x_ref'] = umls_references['x_ref'].str.replace('UMLS:', '') \n",
    "    \n",
    "    # this matches the naming convension for metamap\n",
    "    umls_references = umls_references.rename(columns={'x_ref': 'conceptId'})\n",
    "\n",
    "    return umls_references"
   ]
  },
  {
   "source": [
    "## Graph Functions that Support Comparison"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alt_ids(hpo_code, graph_network):\n",
    "\n",
    "    a = 'alt_id' in graph_network.node[hpo_code]\n",
    "\n",
    "    if a == True:\n",
    "        one_time = graph_network.node[hpo_code]['alt_id']\n",
    "        one_time_dataset = pd.DataFrame(one_time)\n",
    "        one_time_dataset['hpo'] = hpo_code\n",
    "        one_time_dataset.columns = ['alt_hpo', 'hpo']\n",
    "\n",
    "    else:\n",
    "        one_time_dataset = pd.DataFrame()\n",
    "\n",
    "    return one_time_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_alternate_direct_ids(List_of_HPOs, graph_network):\n",
    "    \"\"\"This gets all alternate HPO codes for each given HPO.  \n",
    "    There can be many HPO codes being used for the same phenotype issues.\n",
    "    \n",
    "    Child function:\n",
    "        - get_alt_ids - this supplies the alternate HPO codes for one given HPO code\"\"\"\n",
    "    \n",
    "    final_data = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, len(List_of_HPOs)):\n",
    "        ex_ref = get_alt_ids(List_of_HPOs[i], graph_network)\n",
    "        final_data = final_data.append(ex_ref)\n",
    "\n",
    "    return final_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_child_parent(hpo_code, graph_network):\n",
    "    \"\"\" This function takes one hpo code and returns the parent and child hpos.\n",
    "    \"\"\"\n",
    "\n",
    "        # Internal Function to get the children of an hpo code\n",
    "    def get_child(hpo_code, graph_network):\n",
    "        children = list(graph_network.predecessors(hpo_code))\n",
    "\n",
    "        one = pd.DataFrame(children, columns=['related_hpo'])\n",
    "        one['relationship'] = 'Child'\n",
    "        one['hpo'] = hpo_code\n",
    "\n",
    "        return one\n",
    "\n",
    "        # Internal Function to get the parent of an hpo code\n",
    "    def get_parent(hpo_code, graph_network):\n",
    "        parent = list(graph_network.successors(hpo_code))\n",
    "\n",
    "        one = pd.DataFrame(parent, columns=['related_hpo'])\n",
    "        one['relationship'] = 'Parent'\n",
    "        one['hpo'] = hpo_code\n",
    "\n",
    "        return one \n",
    "\n",
    "    child = get_child(hpo_code, graph_network)\n",
    "    parent = get_parent(hpo_code, graph_network)\n",
    "\n",
    "    full_child_parent = parent.append(child)\n",
    "\n",
    "    full_child_parent = full_child_parent[['related_hpo', 'hpo', 'relationship']]\n",
    "\n",
    "    return full_child_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_child_parent_and_alternatives(hpo_code, graph_network):\n",
    "    \"\"\" Combines the:\n",
    "        1. get_child_parent function, with\n",
    "        2. graph alternate_direct_ids function\n",
    "\n",
    "        To get a full list of all parent and child and thier alternative ids for one hpo code.\n",
    "    \"\"\"\n",
    "    #parent and children\n",
    "    full_child_parent = get_child_parent(hpo_code, graph_network)\n",
    "    \n",
    "    #we now check the parent and children for alternate ids\n",
    "    alt_id_check_list = list(full_child_parent['related_hpo'])\n",
    "    alternate_ids = graph_alternate_direct_ids(alt_id_check_list, graph_network)\n",
    "    \n",
    "    #this is to check if there are alternate ids for any of the relatives\n",
    "    if len(alternate_ids) > 0:\n",
    "        alternate_ids.columns = ['all_hpo_options', 'related_hpo']\n",
    "\n",
    "        merging = full_child_parent.merge(alternate_ids, left_on = 'related_hpo', right_on = 'related_hpo' )\n",
    "\n",
    "        full_child_parent['all_hpo_options'] = full_child_parent['related_hpo']\n",
    "\n",
    "\n",
    "        final_data = full_child_parent.append(merging)\n",
    "    \n",
    "    else:\n",
    "        final_data = full_child_parent\n",
    "        final_data['all_hpo_options'] = final_data['related_hpo']\n",
    "    \n",
    "    final_data = final_data[['all_hpo_options', 'hpo', 'relationship']]\n",
    "    final_data.columns = ['related_hpos_with_alternates', 'hpo', 'relationship']\n",
    "\n",
    "    return final_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_parent_child(dataset, graph_network):\n",
    "    \"\"\"Want to get the HPO codes for each child and parent of a given HPO code. \n",
    "    We also want to get the alternate ids so that we have all of the possible HPO codes that are children and parents.\n",
    "\n",
    "    Child Formula:\n",
    "     - get_child_parent_and_alternatives\n",
    "    \"\"\"\n",
    "     \n",
    "    final_data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0,len(dataset)):\n",
    "        one_row = dataset.iloc[i,:]\n",
    "\n",
    "\n",
    "        ex_ref = get_child_parent_and_alternatives(one_row['hpo'], graph_network)\n",
    "        ex_ref['text_id'] = one_row['text_id']\n",
    "        final_data = final_data.append(ex_ref)\n",
    "    \n",
    "    if len(final_data) == 0:\n",
    "        final_data = pd.DataFrame(columns=['alt_id', 'hpo', 'relationship', 'text_id'])\n",
    "    else:\n",
    "        final_data.columns = ['alt_id', 'hpo', 'relationship', 'text_id'] \n",
    "        \n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Calls and Cleaning for Mondo and MetaMap\n",
    "\n",
    "## Mondo Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_mondo_annotation(text, text_id,  min_word_length = 4, longest_only = 'true', include_abbreviation = 'false', include_acronym = 'false',\n",
    "                            include_numbers = 'false'):\n",
    "\n",
    "    \"\"\"Creates a call to the mondo anotator.  it allows for the same modifications to the mondo annotator as the api itself.\n",
    "    \n",
    "        Requires input of:\n",
    "            - Text to be examined\n",
    "            - ID for the text, most commonly PubMedId\n",
    "    \"\"\"\n",
    "\n",
    "    #needs to be a string to add to the call to the api\n",
    "    min_word_length = str(min_word_length)\n",
    "\n",
    "    call = 'https://api.monarchinitiative.org/api/nlp/annotate/entities/?content=' + text + '&min_length=' + min_word_length + '&longest_only=' + longest_only + '&include_abbreviation=' + include_abbreviation + '&include_acronym=' + include_acronym + '&include_numbers='+ include_numbers\n",
    "    \n",
    "    call = str(call)\n",
    "    \n",
    "    r = requests.get(call)\n",
    "    \n",
    "    # Checking to make sure the request went through to the api sucessfully\n",
    "    if r.status_code == 200:\n",
    "        \n",
    "        x = r.json()\n",
    "        data_call_df = json_normalize(x['spans'], record_path=['token'])\n",
    "        one = data_call_df.reset_index()\n",
    "        one['text_id'] = text_id    \n",
    "        one = one[['id', 'category', 'terms', 'text_id']].astype(str)\n",
    "        one = one.drop_duplicates()\n",
    "    \n",
    "    else:\n",
    "        one = pd.DataFrame(columns = [ 'id', 'category', 'terms', 'text_id'])\n",
    "        \n",
    "    return one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_mondo_annotations(dataset, min_word_length_all = 4, longest_only_all = 'true', \n",
    "    include_abbreviation_all ='false', include_acronym_all = 'false',include_numbers_all = 'false'):\n",
    "\n",
    "    \"\"\" Loops through a dataset with text and IDs. \n",
    "        Requires a dataset with:\n",
    "        - A \"text\" column\n",
    "        - A \"text_id\" column to specify the id for the article\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    mondo_long = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(dataset)):\n",
    "\n",
    "        one_abstract = dataset.iloc[i, :]\n",
    "\n",
    "        #getting mondo_long\n",
    "        one_mondo_long = get_one_mondo_annotation(one_abstract['text'], one_abstract['text_id'],  \n",
    "                                                            min_word_length = min_word_length_all, longest_only = longest_only_all, \n",
    "        include_abbreviation = include_abbreviation_all, include_acronym = include_acronym_all,\n",
    "                                                            include_numbers = include_numbers_all)\n",
    "\n",
    "        mondo_long = mondo_long.append(one_mondo_long)\n",
    "\n",
    "\n",
    "    mondo_long.rename(columns = {'id':'annotation_id'}, inplace = True)   \n",
    "\n",
    "    return mondo_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_mondo_to_hpo(mondo_data_frame):\n",
    "    \"\"\"Makes sure HPO code is being referenced.  \n",
    "        Can be used with one or many annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    mondo_hpo = mondo_data_frame[mondo_data_frame['annotation_id'].str.contains('HP:', na = False)]\n",
    "    #mondo_hpo = mondo_hpo[mondo_hpo['id'].isin(phenotypic_abnormality_hpo)]\n",
    "\n",
    "    mondo_hpo.rename(columns = {'annotation_id':'hpo'}, inplace = True) \n",
    "    return mondo_hpo"
   ]
  },
  {
   "source": [
    "## Metamap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_metamap_annotation(text, text_id):\n",
    "    \"\"\"The metamap api cannot take more than ~1000(?) characters, so we break the text into one sentance each,\n",
    "    then run each sentance through the metamap annotator.\n",
    "    \"\"\"\n",
    "    metamap_test_data = pd.DataFrame()\n",
    "\n",
    "   # MetaMap has a short limit on characters that can be put into the annotator through the api\n",
    "    abstract_sentaces = text.split('. ')\n",
    "   \n",
    "    for abstract_sentace_part in range(len(abstract_sentaces)): # send each sentance through the annotator one by one\n",
    "        \n",
    "        call = 'https://knowledge.ncats.io/ks/umls/metamap/' + abstract_sentaces[abstract_sentace_part]\n",
    "        call = str(call)\n",
    "        r = requests.get(call)\n",
    "        \n",
    "        if r.status_code == 200: # checks if api call is successful\n",
    "            x = r.json()\n",
    "            metamap_test_data_one_example = pd.DataFrame()\n",
    "            for utterance_len in range(len(x['utteranceList'])):  # Each  annotation is nested, so we need to loop through to get all of them\n",
    "                \n",
    "                for pcmlist_len in range(len(x['utteranceList'][utterance_len]['pcmlist'])):\n",
    "                    \n",
    "                    if \"mappingList\" in x['utteranceList'][utterance_len]['pcmlist'][pcmlist_len]:\n",
    "                        \n",
    "                        for mapping in range(len(x['utteranceList'][utterance_len]['pcmlist'][pcmlist_len]['mappingList'])):\n",
    "                            #print('mapping')\n",
    "                            for evList1 in range(len(x['utteranceList'][utterance_len]['pcmlist'][pcmlist_len]['mappingList'][mapping]['evList'])):\n",
    "                                \n",
    "                                one1 = json_normalize(x['utteranceList'][utterance_len]['pcmlist'][pcmlist_len]['mappingList'][mapping]['evList'][evList1])\n",
    "                                metamap_test_data_one_example = metamap_test_data_one_example.append(one1)\n",
    "            metamap_test_data = metamap_test_data.append(metamap_test_data_one_example)\n",
    "    \n",
    "    metamap_test_data['text_id'] = text_id\n",
    "   \n",
    "\n",
    "    return metamap_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metamap_annotations(dataset):\n",
    "    \"\"\" Runs a loop through a full dataset of text and text_ids through the get_one_metamap_annotation function.\"\"\"\n",
    "    metamap = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(dataset)):\n",
    "\n",
    "        one_abstract = dataset.iloc[i, :]\n",
    "\n",
    "        #getting mondo_long\n",
    "        one_metamap = get_one_metamap_annotation(one_abstract['text'], one_abstract['text_id'])\n",
    "\n",
    "        metamap = metamap.append(one_metamap)\n",
    "    \n",
    "    return metamap\n",
    "#removing duplicate values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_metamap_adding_hpo(metamap_data_frame, graph_network):\n",
    "    \"\"\" Function requires a metamap data frame AND the graph network for HPO. This graph network is required\n",
    "        To get all of the umls codes from metamap returned as hpos\n",
    "\n",
    "        This function:\n",
    "        1. Gets the hpo codes for the umls returned in the metamap.\n",
    "        2. Drops duplicate values in the data frame\n",
    "        3. Attached the hpo code to the metamap data frame and removes umls codes that do not have an hpo equivilant.\n",
    "\n",
    "        Uses functions: get_hpos_for_umls_code\n",
    "    \"\"\"\n",
    "\n",
    "    umls_to_hpo = get_hpos_for_umls_code(graph_network)\n",
    "    \n",
    "    metamap_data_frame['semanticTypes'] = metamap_data_frame['semanticTypes'].astype(str)\n",
    "    #drop_dups = metamap_data_frame.drop_duplicates(['conceptId', 'pmid'])\n",
    "    drop_dups = metamap_data_frame.drop_duplicates(['conceptId', 'text_id', 'semanticTypes' ])\n",
    "    \n",
    "    metamap_hpo = drop_dups.merge(umls_to_hpo, how = 'left', on = 'conceptId')\n",
    "    metamap_hpo = metamap_hpo[~metamap_hpo['hpo'].isnull()]\n",
    "\n",
    "    #changing text_id to \n",
    "    metamap_hpo['text_id'] = metamap_hpo['text_id'].astype(str)\n",
    "\n",
    "    return metamap_hpo"
   ]
  },
  {
   "source": [
    "# Comparison of Known Annotations and Data to Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_annotation_direct_matching(one_hpo_annotation, one_annotation_to_test, graph_network):\n",
    "\n",
    "    \"\"\" \n",
    "    In this function, we take one set of known hpo annotations and one set of annotations to test against this known set. For this test set, we check for hpo matches directly and with alternate ids.\n",
    "\n",
    "    Input: known_annotations dataset (with at least unqiueid and hpo columns); dataset of annotations to test (with at least hpo and text_id columns); graph network for hpo; graph network\n",
    "    Output: dataset with exact matches, annotations to test that do not have matches, known hpo annotations that do not have a match.\n",
    "\n",
    "    Child functions: \n",
    "    1. graph_alternate_direct_ids\n",
    "    \"\"\"\n",
    "\n",
    "    #Testing up up the list\n",
    "    data_to_test = one_annotation_to_test[['hpo', 'text_id']]\n",
    "    data_to_test['hpo'] = data_to_test['hpo'].astype(str)\n",
    "    data_to_test['exact_match'] = 1\n",
    "    data_to_test['alt_id'] = 0\n",
    "    data_to_test['test_set_annotations_with_no_match'] = 0\n",
    "\n",
    "\n",
    "    # We want to determine which of our test set hpos are an exact match for the test data\n",
    "    exact_match = one_hpo_annotation.merge(data_to_test, how = 'left', on =['hpo'])#, 'pmid'])#, 'uniqueid'])\n",
    "    exact_match = exact_match[exact_match['exact_match'] == 1]\n",
    "\n",
    "    ##############\n",
    "    #TESTING ALTERNATE ID\n",
    "    ##############\n",
    "    #create the new annotations and test data sets without the hpos that are part of the exact match\n",
    "    alternate_id = one_hpo_annotation[~one_hpo_annotation['uniqueid'].isin(exact_match['uniqueid'])]\n",
    "    test_group_for_alt_id = data_to_test[~data_to_test['hpo'].isin(exact_match['hpo'])]\n",
    "\n",
    "    alt_id = graph_alternate_direct_ids(test_group_for_alt_id['hpo'].tolist(), graph_network)\n",
    "\n",
    "    # Sometimes there are no alternate ids\n",
    "    if len(alt_id) == 0:\n",
    "        exact_alt_match = pd.DataFrame()\n",
    "    else:\n",
    "        alt_id['exact_match'] = 0\n",
    "        alt_id['alt_id'] = 1\n",
    "        alt_id['test_set_annotations_with_no_match'] = 0\n",
    "        alt_id = alt_id.merge(test_group_for_alt_id[['hpo', 'text_id']], how = 'left', on = 'hpo') # adding back in the text_id\n",
    "        alt_id_final = alt_id[['alt_hpo','exact_match', 'alt_id', 'test_set_annotations_with_no_match', 'text_id']]\n",
    "\n",
    "        exact_alt_match = alternate_id.merge(alt_id_final , how = 'left', left_on = 'hpo', right_on = 'alt_hpo')\n",
    "        exact_alt_match = exact_alt_match[exact_alt_match['alt_id'] == 1]\n",
    "        exact_alt_match = exact_alt_match.drop('alt_hpo' , axis='columns')\n",
    "\n",
    "    ##################\n",
    "    #NO MATCH\n",
    "    ###################\n",
    "    #we still want to capture hpos from the test group that don't match any of the known hpos\n",
    "    no_match = data_to_test[~data_to_test['hpo'].isin(exact_match['hpo'])]\n",
    "\n",
    "        # Sometimes there are no alternate ids\n",
    "    if len(alt_id) == 0:\n",
    "        none = 1\n",
    "        #skip this step if there is no alternate ids\n",
    "    else:\n",
    "        no_match = no_match[~no_match['hpo'].isin(exact_alt_match['hpo'])]\n",
    "   \n",
    "    no_match['exact_match'] = 0\n",
    "    no_match['alt_id'] = 0\n",
    "    no_match['test_set_annotations_with_no_match'] = 1\n",
    "    #test_parent_child_final = test_parent_child[['alt_hpo_code','exact_match', 'alt_id', 'test_set_annotations_with_no_match', 'text_id']]\n",
    "\n",
    "    ##############################################\n",
    "    #COMBINING ALL OF THE VALUES FROM TESTS ABOVE\n",
    "    ###############################################\n",
    "\n",
    "    combined_findings = exact_match\n",
    "    combined_findings = combined_findings.append(exact_alt_match)\n",
    "    combined_findings = combined_findings.append(no_match)\n",
    "\n",
    "    #combining direct and alternate id since they both represent exact matches\n",
    "    combined_findings['direct_alt'] = combined_findings['exact_match'] + combined_findings['alt_id']\n",
    "    combined_findings['exact_match'] = np.where(combined_findings['direct_alt'] >= 1, 1, 0)\n",
    "\n",
    "    combined_findings = combined_findings[['uniqueid', 'hpo', 'text_id', 'exact_match', 'test_set_annotations_with_no_match']]\n",
    "    combined_findings['known_annotations_with_no_match'] = 0\n",
    "  \n",
    "    #####################\n",
    "    ### ORGANIZING Known annotations that were not selected\n",
    "\n",
    "    remaining_from_one_hpo_annotation = one_hpo_annotation[~one_hpo_annotation['uniqueid'].isin(combined_findings['uniqueid'])]\n",
    "    remaining_from_one_hpo_annotation['exact_match'] = 0\n",
    "    remaining_from_one_hpo_annotation['test_set_annotations_with_no_match'] = 0\n",
    "    remaining_from_one_hpo_annotation['known_annotations_with_no_match'] = 1\n",
    "    remaining_from_one_hpo_annotation['text_id'] =  np.NaN\n",
    "\n",
    "    remaining_from_one_hpo_annotation = remaining_from_one_hpo_annotation[['uniqueid', 'hpo', 'text_id', 'exact_match', 'test_set_annotations_with_no_match','known_annotations_with_no_match']]\n",
    "\n",
    "    final_data = combined_findings.append(remaining_from_one_hpo_annotation)\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hpo_annotation = hpo_annotations\n",
    "#one_annotation_to_test = one_mondo_annotation\n",
    "#graph_network = g\n",
    "def one_annotation_matching_relatives(one_hpo_annotation, one_annotation_to_test, graph_network):\n",
    "\n",
    "    \"\"\" \n",
    "    In this function, we take one set of known hpo annotations and one set of annotations to test against this known set. For this test set, we check for hpo matches directly, with alternate ids, and finally, with any parent, child relationships (along with thier alternate ids).\n",
    "\n",
    "    Input: known_annotations dataset (with at least unqiueid and hpo columns); dataset of annotations to test (with at least hpo and text_id columns); graph network for hpo; graph network\n",
    "    Output: dataset with exact matches, matches to relatives, annotations to test that do not have matches, known hpo annotations that do not have a match.\n",
    "\n",
    "    Child functions: \n",
    "    1. graph_alternate_direct_ids\n",
    "    2. graph_parent_child\n",
    "    \"\"\"\n",
    "\n",
    "    #Testing up up the list\n",
    "    data_to_test = one_annotation_to_test[['hpo', 'text_id']]\n",
    "    data_to_test['hpo'] = data_to_test['hpo'].astype(str)\n",
    "    data_to_test['exact_match'] = 1\n",
    "    data_to_test['alt_id'] = 0\n",
    "    data_to_test['relative_match'] = 0\n",
    "    data_to_test['test_set_annotations_with_no_match'] = 0\n",
    "\n",
    "\n",
    "    # We want to determine which of our test set hpos are an exact match for the test data\n",
    "    exact_match = one_hpo_annotation.merge(data_to_test, how = 'left', on =['hpo'])#, 'pmid'])#, 'uniqueid'])\n",
    "    exact_match = exact_match[exact_match['exact_match'] == 1]\n",
    "\n",
    "    ##############\n",
    "    #TESTING ALTERNATE ID\n",
    "    ##############\n",
    "    #create the new annotations and test data sets without the hpos that are part of the exact match\n",
    "    alternate_id = one_hpo_annotation[~one_hpo_annotation['uniqueid'].isin(exact_match['uniqueid'])]\n",
    "    test_group_for_alt_id = data_to_test[~data_to_test['hpo'].isin(exact_match['hpo'])]\n",
    "\n",
    "    alt_id = graph_alternate_direct_ids(test_group_for_alt_id['hpo'].tolist(), graph_network)\n",
    "    \n",
    "    # Sometimes there are no alternate ids\n",
    "    if len(alt_id) == 0:\n",
    "        exact_alt_match = pd.DataFrame()\n",
    "    else:\n",
    "        alt_id['exact_match'] = 0\n",
    "        alt_id['alt_id'] = 1\n",
    "        alt_id['test_set_annotations_with_no_match'] = 0\n",
    "        alt_id = alt_id.merge(test_group_for_alt_id[['hpo', 'text_id']], how = 'left', on = 'hpo') # adding back in the text_id\n",
    "        alt_id_final = alt_id[['alt_hpo','exact_match', 'alt_id', 'test_set_annotations_with_no_match', 'text_id']]\n",
    "\n",
    "        exact_alt_match = alternate_id.merge(alt_id_final , how = 'left', left_on = 'hpo', right_on = 'alt_hpo')\n",
    "        exact_alt_match = exact_alt_match[exact_alt_match['alt_id'] == 1]\n",
    "        exact_alt_match = exact_alt_match.drop('alt_hpo' , axis='columns')\n",
    "\n",
    "\n",
    "    ####################\n",
    "    ## ADD IN TESTING OF RELATIVES\n",
    "    #################\n",
    "    #now we need to remove any hpos that have been found in the exact match or alternate id process\n",
    "    relatives_id = one_hpo_annotation[~one_hpo_annotation['uniqueid'].isin(exact_match['uniqueid'])]\n",
    "\n",
    "    # Sometimes there are no alternate ids\n",
    "    if len(alt_id) == 0:\n",
    "        none = 1\n",
    "        #skip this step if there is no alternate ids\n",
    "    else:\n",
    "        relatives_id = relatives_id[~relatives_id['uniqueid'].isin(exact_alt_match['uniqueid'])]\n",
    "\n",
    "    #remove hpos from the test group that have been found already\n",
    "    test_group_for_relatives = data_to_test[~data_to_test['hpo'].isin(exact_match['hpo'])]\n",
    "    \n",
    "        # Sometimes there are no alternate ids\n",
    "    if len(alt_id) == 0:\n",
    "        none = 1\n",
    "        #skip this step if there is no alternate ids\n",
    "    else:\n",
    "        test_group_for_relatives = test_group_for_relatives[~test_group_for_relatives['hpo'].isin(exact_alt_match['hpo'])]\n",
    "    \n",
    "\n",
    "    #This code checks the test group hpos against children and parent hpos. It also checks against the alternative ids\n",
    "    test_parent_child = graph_parent_child(test_group_for_relatives, graph_network)\n",
    "    \n",
    "    test_parent_child.rename(columns={'alt_id':'alt_hpo_code', 'hpo': 'original_hpo'}, inplace=True)\n",
    "    test_parent_child['exact_match'] = 0\n",
    "    test_parent_child['alt_id'] = 0\n",
    "    test_parent_child['relative_match'] = 1\n",
    "    test_parent_child['test_set_annotations_with_no_match'] = 0\n",
    "    test_parent_child_final = test_parent_child[['alt_hpo_code','exact_match', 'alt_id', 'relative_match', 'test_set_annotations_with_no_match','text_id','original_hpo']]\n",
    "\n",
    "    # Combine the two datasets to test \n",
    "    exact_relative_match = relatives_id.merge(test_parent_child_final, how = 'left', left_on = 'hpo', right_on = 'alt_hpo_code' )\n",
    "    exact_relative_match = exact_relative_match[exact_relative_match['relative_match'] == 1]\n",
    "    exact_relative_match = exact_relative_match.drop('alt_hpo_code' , axis='columns')\n",
    "\n",
    "    ##################\n",
    "    #NO MATCH\n",
    "    ###################\n",
    "    #we still want to capture hpos from the test group that don't match any of the known hpos\n",
    "    no_match = data_to_test[~data_to_test['hpo'].isin(exact_match['hpo'])]\n",
    "    \n",
    "    no_match = no_match[~no_match['hpo'].isin(exact_relative_match['original_hpo'])]\n",
    "\n",
    "    # Sometimes there are no alternate ids\n",
    "    if len(alt_id) == 0:\n",
    "        none = 1\n",
    "        #skip this step if there is no alternate ids\n",
    "    else:\n",
    "        no_match = no_match[~no_match['hpo'].isin(exact_alt_match['hpo'])]\n",
    "\n",
    "\n",
    "    no_match['exact_match'] = 0\n",
    "    no_match['alt_id'] = 0\n",
    "    no_match['relative_match'] = 0\n",
    "    no_match['test_set_annotations_with_no_match'] = 1\n",
    "    #test_parent_child_final = test_parent_child[['alt_hpo_code','exact_match', 'alt_id', 'relative_match', 'test_set_annotations_with_no_match', 'text_id']]\n",
    "\n",
    "    ##############################################\n",
    "    #COMBINING ALL OF THE VALUES FROM TESTS ABOVE\n",
    "    ###############################################\n",
    "\n",
    "    combined_findings = exact_match\n",
    "    combined_findings = combined_findings.append(exact_alt_match)\n",
    "    combined_findings = combined_findings.append(exact_relative_match)\n",
    "    combined_findings = combined_findings.append(no_match)\n",
    "\n",
    "    #combining direct and alternate id since they both represent exact matches\n",
    "    combined_findings['direct_alt'] = combined_findings['exact_match'] + combined_findings['alt_id']\n",
    "    combined_findings['exact_match'] = np.where(combined_findings['direct_alt'] >= 1, 1, 0)\n",
    "\n",
    "    combined_findings = combined_findings[['uniqueid', 'hpo', 'original_hpo', 'text_id', 'exact_match', 'relative_match', 'test_set_annotations_with_no_match']]\n",
    "    combined_findings['known_annotations_with_no_match'] = 0\n",
    "\n",
    "    #####################\n",
    "    ### ORGANIZING Known annotations that were not selected\n",
    "\n",
    "    remaining_from_one_hpo_annotation = one_hpo_annotation[~one_hpo_annotation['uniqueid'].isin(combined_findings['uniqueid'])]\n",
    "    remaining_from_one_hpo_annotation['exact_match'] = 0\n",
    "    remaining_from_one_hpo_annotation['relative_match'] = 0\n",
    "    remaining_from_one_hpo_annotation['test_set_annotations_with_no_match'] = 0\n",
    "    remaining_from_one_hpo_annotation['known_annotations_with_no_match'] = 1\n",
    "    remaining_from_one_hpo_annotation['text_id'] =  np.NaN\n",
    "    remaining_from_one_hpo_annotation['original_hpo'] =  np.NaN\n",
    "\n",
    "    remaining_from_one_hpo_annotation = remaining_from_one_hpo_annotation[['uniqueid', 'hpo','original_hpo', 'text_id', 'exact_match', 'relative_match', 'test_set_annotations_with_no_match','known_annotations_with_no_match']]\n",
    "\n",
    "    final_data = combined_findings.append(remaining_from_one_hpo_annotation)\n",
    "    #combining the exact and relative match\n",
    "\n",
    "    #final_data['exact_and_relative_match'] = np.where((final_data['exact_match'] == 1) | (final_data['relative_match'] == 1), 1, 0)\n",
    "    #final_data_df = final_data[['']]\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "source": [
    "# Scoring Process for Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(dataset):\n",
    "\n",
    "     \"\"\"\n",
    "     This function scores the datasets produced in comparing the test data to known annotators.\n",
    "     \"\"\"\n",
    "\n",
    "# Just indicated whether relatives were included\n",
    "     if 'relative_match' in dataset:\n",
    "          ScoringType = 'Relative'\n",
    "     else :\n",
    "          ScoringType = 'Direct'\n",
    "\n",
    "     #We need to change the acutally false depending on whether we're counting exact matches or relatives are included\n",
    "     dataset['predicted_true'] = np.where(~ pd.isna(dataset['text_id']), 1, 0)\n",
    "     dataset['actually_true'] = np.where(~ pd.isna(dataset['uniqueid']), 1, 0)\n",
    "     #dataset['actually_false'] = np.where(pd.isna(dataset['uniqueid']), 1, 0)\n",
    "\n",
    "     #confusion matrix\n",
    "     dataset['true_positive'] = np.where((dataset['predicted_true'] == 1) & (dataset['actually_true'] == 1), 1, 0) \n",
    "     dataset['false_positive'] = np.where((dataset['predicted_true'] == 1) & (dataset['actually_true'] == 0), 1, 0)\n",
    "     dataset['false_negative'] = np.where((dataset['predicted_true'] == 0) & (dataset['actually_true'] == 1), 1, 0)\n",
    "     dataset['true_negative'] = 0 #we don't have any true negatives since our annotators are not predicting what is NOT there.\n",
    "     \n",
    "     true_positive = dataset['true_positive'].sum()\n",
    "     false_positive = dataset['false_positive'].sum()\n",
    "     false_negative = dataset['false_negative'].sum()\n",
    "     true_negative = dataset['true_negative'].sum()\n",
    "\n",
    "\n",
    "     #basic statistics\n",
    "     number_of_annotations_to_test = dataset['predicted_true'].sum()\n",
    "     known_annotations_to_test_against = dataset['actually_true'].sum()\n",
    "     accurately_predicted = dataset['true_positive'].sum()\n",
    "\n",
    "     #Measures\n",
    "     #accuracy = metrics.accuracy_score(dataset['actually_true'], dataset['predicted_true'])\n",
    "     precision = metrics.precision_score(dataset['actually_true'], dataset['predicted_true'])\n",
    "     recall = metrics.recall_score(dataset['actually_true'], dataset['predicted_true'])\n",
    "     f1_score = metrics.f1_score(dataset['actually_true'], dataset['predicted_true'])\n",
    "\n",
    "\n",
    "\n",
    "     final_data = {'ScoringType': ScoringType, 'Annotations_to_Test': [number_of_annotations_to_test],\n",
    "                    'Known_Annotations': [known_annotations_to_test_against],\n",
    "                    'Accurately_Predicted': [accurately_predicted],\n",
    "                    #'Accuracy': [accuracy],\n",
    "                    'Precision': [precision],\n",
    "                    'Recall': [recall],\n",
    "                    'F1_Score': [f1_score],\n",
    "                    'True_Positive': true_positive,\n",
    "                    'False_Positive': false_positive,\n",
    "                     'False_Negative': false_negative,\n",
    "                    'True_Negative': true_negative}\n",
    "\n",
    "     final_data_df = pd.DataFrame(final_data)\n",
    " \n",
    "\n",
    "     return final_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}