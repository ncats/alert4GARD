{
 "cells": [
  {
   "source": [
    "# Load functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:/Users/asclo/Desktop/HHS/NIH Dashboard/Python Notebooks/Final Deliverables/FunctionsForAnalysis.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_matching_source(known_annotations, annotations_to_test, graph_network):\n",
    "\n",
    "    \"\"\"  This allows you to test a dataset of annotations, with multiple text_ids, against\n",
    "\n",
    "        This ONLY tests exact matches in the text_id and the references in the known annotations.\n",
    "\n",
    "        Outputs are as follows:\n",
    "        - output[0] - exact matches between the test annotators and known annotators\n",
    "        - output[1] - exact and relational matches between the test annotators and known annotators\n",
    "        - output[2] - Any annotations we tried to test, but did not have known annotators for\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    unique_text_sources = annotations_to_test['text_id'].drop_duplicates()\n",
    "\n",
    "    #one dataset for direct matches and one of matches with relatives\n",
    "    exact_final_data = pd.DataFrame()\n",
    "    related_final_data = pd.DataFrame()\n",
    "    \n",
    "    list_for_issues = []\n",
    "\n",
    "    for i in unique_text_sources:\n",
    "        #one_source = unique_text_sources.iloc[i]\n",
    "        one_test_set = annotations_to_test[annotations_to_test['text_id'] == i]\n",
    "\n",
    "        one_source_for_known_annotators =  'PMID:' + i \n",
    "\n",
    "        one_known_set = known_annotations[known_annotations['reference'] == one_source_for_known_annotators]\n",
    "\n",
    "\n",
    "        if len(one_known_set) == 0:\n",
    "            \n",
    "            list_for_issues.append(i)\n",
    "\n",
    "        else:\n",
    "\n",
    "            exact = one_annotation_direct_matching(one_known_set, one_test_set,  graph_network)\n",
    "            exact['matching_to'] = i\n",
    "            exact_final_data = exact_final_data.append(exact)\n",
    "\n",
    "            related = one_annotation_matching_relatives(one_known_set, one_test_set, graph_network)\n",
    "            related['matching_to'] = i\n",
    "            related_final_data = related_final_data.append(related)\n",
    "            \n",
    "    \n",
    "    return [exact_final_data, related_final_data, list_for_issues]\n"
   ]
  },
  {
   "source": [
    "### Explaination of the testing_matching_source function\n",
    "\n",
    "This function has a loop that takes each pubmed_id, extracts the hpo code assocaited directly with that pubmed_id, and compares themto:\n",
    "- one_annotation_direct_matching function for exact matches\n",
    "- one_annotation_matching_relatives function for exact and related matches\n",
    "\n",
    "Both of these datasets can be inputed into the scoring function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Run Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph database\n",
    "g = hpo_hierarchy_graph_load()\n",
    "phenotypic_abnormality_hpos = graph_phenotypic_abnormality(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  #disease-db  disease-identifier              disease-name  negation  \\\n",
       "0    DECIPHER                   1  Wolf-Hirschhorn Syndrome       NaN   \n",
       "1    DECIPHER                   1  Wolf-Hirschhorn Syndrome       NaN   \n",
       "\n",
       "          hpo   reference evidence-code onset frequencyHPO modifier  \\\n",
       "0  HP:0000252  DECIPHER:1           IEA   NaN          NaN            \n",
       "1  HP:0001249  DECIPHER:1           IEA   NaN          NaN            \n",
       "\n",
       "  sub-ontology                 alt-names                  curators  \\\n",
       "0            P  WOLF-HIRSCHHORN SYNDROME  HPO:skoehler[2013-05-29]   \n",
       "1            P  WOLF-HIRSCHHORN SYNDROME  HPO:skoehler[2013-05-29]   \n",
       "\n",
       "  frequencyRaw sex              uniqueid  \n",
       "0            -   -  HP:0000252DECIPHER:1  \n",
       "1            -   -  HP:0001249DECIPHER:1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#disease-db</th>\n      <th>disease-identifier</th>\n      <th>disease-name</th>\n      <th>negation</th>\n      <th>hpo</th>\n      <th>reference</th>\n      <th>evidence-code</th>\n      <th>onset</th>\n      <th>frequencyHPO</th>\n      <th>modifier</th>\n      <th>sub-ontology</th>\n      <th>alt-names</th>\n      <th>curators</th>\n      <th>frequencyRaw</th>\n      <th>sex</th>\n      <th>uniqueid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>DECIPHER</td>\n      <td>1</td>\n      <td>Wolf-Hirschhorn Syndrome</td>\n      <td>NaN</td>\n      <td>HP:0000252</td>\n      <td>DECIPHER:1</td>\n      <td>IEA</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td></td>\n      <td>P</td>\n      <td>WOLF-HIRSCHHORN SYNDROME</td>\n      <td>HPO:skoehler[2013-05-29]</td>\n      <td>-</td>\n      <td>-</td>\n      <td>HP:0000252DECIPHER:1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>DECIPHER</td>\n      <td>1</td>\n      <td>Wolf-Hirschhorn Syndrome</td>\n      <td>NaN</td>\n      <td>HP:0001249</td>\n      <td>DECIPHER:1</td>\n      <td>IEA</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td></td>\n      <td>P</td>\n      <td>WOLF-HIRSCHHORN SYNDROME</td>\n      <td>HPO:skoehler[2013-05-29]</td>\n      <td>-</td>\n      <td>-</td>\n      <td>HP:0001249DECIPHER:1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# HPO annotations file- Our current Gold Standard\n",
    "hpo_known_annotations = get_hpo_annotations_and_clean()\n",
    "hpo_known_annotations = hpo_known_annotations[hpo_known_annotations['hpo'].isin(phenotypic_abnormality_hpos)]\n",
    "hpo_known_annotations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    text_id                                              Title  \\\n",
       "0  17088400  Recurrent infections, hypotonia, and mental re...   \n",
       "1  19133692  15q overgrowth syndrome: a newly recognized ph...   \n",
       "2  10951463  Tetrasomy 15q25-->qter: cytogenetic and molecu...   \n",
       "\n",
       "                                                text  \\\n",
       "0  OBJECTIVE: Our goal was to describe the neurol...   \n",
       "1  Trisomy and tetrasomy of distal chromosome 15q...   \n",
       "2  Tetrasomy for the distal long arm of chromosom...   \n",
       "\n",
       "                                             PubType  \n",
       "0  research support, non-u.s. gov't; journal arti...  \n",
       "1                                    journal article  \n",
       "2                      journal article; case reports  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>Title</th>\n      <th>text</th>\n      <th>PubType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>17088400</td>\n      <td>Recurrent infections, hypotonia, and mental re...</td>\n      <td>OBJECTIVE: Our goal was to describe the neurol...</td>\n      <td>research support, non-u.s. gov't; journal arti...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>19133692</td>\n      <td>15q overgrowth syndrome: a newly recognized ph...</td>\n      <td>Trisomy and tetrasomy of distal chromosome 15q...</td>\n      <td>journal article</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>10951463</td>\n      <td>Tetrasomy 15q25--&gt;qter: cytogenetic and molecu...</td>\n      <td>Tetrasomy for the distal long arm of chromosom...</td>\n      <td>journal article; case reports</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "#Abstracts to Annotate\n",
    "abstracts = pd.read_csv('C:/Users/asclo/Desktop/HHS/NIH Dashboard/Python Notebooks/Final Deliverables/All pmid abstracts from hpo annotations.csv',sep=',', index_col = False)\n",
    "abstracts = abstracts.rename(columns = {'Pubmed_ID': 'text_id', 'Abstract': 'text'})\n",
    "\n",
    "# pubmed ids that we know are case reports\n",
    "# Qian provided this list\n",
    "case_studies_pmid = pd.read_csv('C:/Users/asclo/Desktop/HHS/NIH Dashboard/Python Notebooks/Final Deliverables/Step1_extra - Full Qian Case Study List.csv', sep=',', index_col = False)\n",
    "\n",
    "abstracts_for_analysis = abstracts[abstracts['text_id'].astype(int).isin(case_studies_pmid['PMID'])]\n",
    "\n",
    "#making sure abstracts are in the known annotaions\n",
    "abstracts_for_analysis['pmid'] = 'PMID:' + abstracts_for_analysis['text_id'].astype(str)\n",
    "abstracts_for_analysis = abstracts_for_analysis[abstracts_for_analysis['pmid'].isin(hpo_known_annotations['reference'])]\n",
    "\n",
    "abstracts_for_analysis = abstracts.iloc[0:100, :]\n",
    "\n",
    "abstracts_for_analysis.head(3)"
   ]
  },
  {
   "source": [
    "# Mondo Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mondo_example = get_all_mondo_annotations(abstracts_for_analysis)\n",
    "mondo_example = cleaning_mondo_to_hpo(mondo_example)\n",
    "\n",
    "#We want to make sure the sample hpos are within phenotypic_abnormality\n",
    "mondo_example = mondo_example[mondo_example['hpo'].isin(phenotypic_abnormality_hpos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = testing_matching_source(hpo_known_annotations, mondo_example, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0      Direct                  304                460                    89   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.131852   0.292763  0.193478  0.232984             89             215   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0             371              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Direct</td>\n      <td>304</td>\n      <td>460</td>\n      <td>89</td>\n      <td>0.131852</td>\n      <td>0.292763</td>\n      <td>0.193478</td>\n      <td>0.232984</td>\n      <td>89</td>\n      <td>215</td>\n      <td>371</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "scoring(one[0]) # annotations that search for exact matches between hpo codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0    Relative                  305                460                    99   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.148649    0.32459  0.215217  0.258824             99             206   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0             361              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Relative</td>\n      <td>305</td>\n      <td>460</td>\n      <td>99</td>\n      <td>0.148649</td>\n      <td>0.32459</td>\n      <td>0.215217</td>\n      <td>0.258824</td>\n      <td>99</td>\n      <td>206</td>\n      <td>361</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "scoring(one[1]) # annotations that search for exact as well as child and parent matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['9171831', '9781021']"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "one[2] # any text ids that did not match a known annotator reference"
   ]
  },
  {
   "source": [
    "# MetaMap Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaMap Example\n",
    "metamap_example = get_all_metamap_annotations(abstracts_for_analysis)\n",
    "metamap_example = cleaning_metamap_adding_hpo(metamap_example, g)\n",
    "\n",
    "#We want to make sure the sample hpos are within phenotypic_abnormality\n",
    "metamap_example = metamap_example[metamap_example['hpo'].isin(phenotypic_abnormality_hpos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = testing_matching_source(hpo_known_annotations, metamap_example, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0      Direct                  365                479                    84   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.110526   0.230137  0.175365  0.199052             84             281   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0             395              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Direct</td>\n      <td>365</td>\n      <td>479</td>\n      <td>84</td>\n      <td>0.110526</td>\n      <td>0.230137</td>\n      <td>0.175365</td>\n      <td>0.199052</td>\n      <td>84</td>\n      <td>281</td>\n      <td>395</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "scoring(one[0]) # annotations that search for exact matches between hpo codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0    Relative                  368                480                   108   \n",
       "\n",
       "   Accuracy  Precision  Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.145946   0.293478   0.225  0.254717            108             260   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0             372              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Relative</td>\n      <td>368</td>\n      <td>480</td>\n      <td>108</td>\n      <td>0.145946</td>\n      <td>0.293478</td>\n      <td>0.225</td>\n      <td>0.254717</td>\n      <td>108</td>\n      <td>260</td>\n      <td>372</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "scoring(one[1]) # annotations that search for child and parent matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['21325395', '9171831', '9781021']"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "one[2]"
   ]
  },
  {
   "source": [
    "# Testing the MetaMap Categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaMap Example\n",
    "metamap_example = get_all_metamap_annotations(abstracts_for_analysis)\n",
    "metamap_example = cleaning_metamap_adding_hpo(metamap_example, g)\n",
    "\n",
    "#We want to make sure the sample hpos are within phenotypic_abnormality\n",
    "metamap_example = metamap_example[metamap_example['hpo'].isin(phenotypic_abnormality_hpos)]\n",
    "\n",
    "\n",
    "# dsyn Disease or Syndrome\n",
    "dsyn = metamap_example[metamap_example['semanticTypes'].apply(lambda x: 'dsyn' in x)]\n",
    "# fndg Finding\n",
    "fndg = metamap_example[metamap_example['semanticTypes'].apply(lambda x: 'fndg' in x)]\n",
    "# sosy Sign or Symptom\n",
    "sosy = metamap_example[metamap_example['semanticTypes'].apply(lambda x: 'sosy' in x)]"
   ]
  },
  {
   "source": [
    "### Testing dsyn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0      Direct                  111                221                    21   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.067524   0.189189  0.095023  0.126506             21              90   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0             200              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Direct</td>\n      <td>111</td>\n      <td>221</td>\n      <td>21</td>\n      <td>0.067524</td>\n      <td>0.189189</td>\n      <td>0.095023</td>\n      <td>0.126506</td>\n      <td>21</td>\n      <td>90</td>\n      <td>200</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "dsyn_testing = testing_matching_source(hpo_known_annotations, dsyn, g)\n",
    "scoring(dsyn_testing[0]) # annotations that search for exact matches between hpo codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0    Relative                  111                221                    32   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.106667   0.288288  0.144796  0.192771             32              79   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0             189              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Relative</td>\n      <td>111</td>\n      <td>221</td>\n      <td>32</td>\n      <td>0.106667</td>\n      <td>0.288288</td>\n      <td>0.144796</td>\n      <td>0.192771</td>\n      <td>32</td>\n      <td>79</td>\n      <td>189</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "scoring(dsyn_testing[1])  # annotations that search for child and parent matches"
   ]
  },
  {
   "source": [
    "### Testing fndg"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0      Direct                   69                304                    22   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.062678   0.318841  0.072368  0.117962             22              47   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0             282              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Direct</td>\n      <td>69</td>\n      <td>304</td>\n      <td>22</td>\n      <td>0.062678</td>\n      <td>0.318841</td>\n      <td>0.072368</td>\n      <td>0.117962</td>\n      <td>22</td>\n      <td>47</td>\n      <td>282</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "fndg_testing = testing_matching_source(hpo_known_annotations, fndg, g)\n",
    "scoring(fndg_testing[0]) # annotations that search for exact matches between hpo codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0    Relative                   70                304                    25   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.071633   0.357143  0.082237   0.13369             25              45   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0             279              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Relative</td>\n      <td>70</td>\n      <td>304</td>\n      <td>25</td>\n      <td>0.071633</td>\n      <td>0.357143</td>\n      <td>0.082237</td>\n      <td>0.13369</td>\n      <td>25</td>\n      <td>45</td>\n      <td>279</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "scoring(fndg_testing[1])  # annotations that search for child and parent matches"
   ]
  },
  {
   "source": [
    "### Testing sosy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0      Direct                   18                 83                     5   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.052083   0.277778  0.060241   0.09901              5              13   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0              78              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Direct</td>\n      <td>18</td>\n      <td>83</td>\n      <td>5</td>\n      <td>0.052083</td>\n      <td>0.277778</td>\n      <td>0.060241</td>\n      <td>0.09901</td>\n      <td>5</td>\n      <td>13</td>\n      <td>78</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "sosy_testing = testing_matching_source(hpo_known_annotations, sosy, g)\n",
    "scoring(sosy_testing[0]) # annotations that search for exact matches between hpo codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  ScoringType  Annotations_to_Test  Known_Annotations  Accurately_Predicted  \\\n",
       "0    Relative                   18                 83                     6   \n",
       "\n",
       "   Accuracy  Precision    Recall  F1_Score  True_Positive  False_Positive  \\\n",
       "0  0.063158   0.333333  0.072289  0.118812              6              12   \n",
       "\n",
       "   False_Negative  True_Negative  \n",
       "0              77              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ScoringType</th>\n      <th>Annotations_to_Test</th>\n      <th>Known_Annotations</th>\n      <th>Accurately_Predicted</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_Score</th>\n      <th>True_Positive</th>\n      <th>False_Positive</th>\n      <th>False_Negative</th>\n      <th>True_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Relative</td>\n      <td>18</td>\n      <td>83</td>\n      <td>6</td>\n      <td>0.063158</td>\n      <td>0.333333</td>\n      <td>0.072289</td>\n      <td>0.118812</td>\n      <td>6</td>\n      <td>12</td>\n      <td>77</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "scoring(sosy_testing[1])  # annotations that search for child and parent matches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}